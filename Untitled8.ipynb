{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programdata\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "D:\\Programdata\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py:46: DeprecationWarning: OpenSSL.rand is deprecated - you should use os.urandom instead\n",
      "  import OpenSSL.SSL\n"
     ]
    }
   ],
   "source": [
    "from mxnet import gluon\n",
    "\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(256, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(10))\n",
    "net.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 1.270856, Train acc 0.623397, Test acc 0.710036\n",
      "Epoch 1. Loss: 0.773002, Train acc 0.741036, Test acc 0.763421\n",
      "Epoch 2. Loss: 0.666519, Train acc 0.775992, Test acc 0.784655\n",
      "Epoch 3. Loss: 0.609354, Train acc 0.794989, Test acc 0.804587\n",
      "Epoch 4. Loss: 0.571838, Train acc 0.807475, Test acc 0.813001\n",
      "Epoch 5. Loss: 0.545754, Train acc 0.816223, Test acc 0.819211\n",
      "Epoch 6. Loss: 0.526226, Train acc 0.821231, Test acc 0.823618\n",
      "Epoch 7. Loss: 0.510700, Train acc 0.825955, Test acc 0.831330\n",
      "Epoch 8. Loss: 0.497407, Train acc 0.829761, Test acc 0.830729\n",
      "Epoch 9. Loss: 0.486606, Train acc 0.833383, Test acc 0.835537\n",
      "Epoch 10. Loss: 0.477358, Train acc 0.835771, Test acc 0.837139\n",
      "Epoch 11. Loss: 0.470084, Train acc 0.837223, Test acc 0.843950\n",
      "Epoch 12. Loss: 0.462762, Train acc 0.840645, Test acc 0.841246\n",
      "Epoch 13. Loss: 0.456306, Train acc 0.841964, Test acc 0.844050\n",
      "Epoch 14. Loss: 0.450159, Train acc 0.844101, Test acc 0.845853\n",
      "Epoch 15. Loss: 0.444932, Train acc 0.845887, Test acc 0.848458\n",
      "Epoch 16. Loss: 0.440199, Train acc 0.847489, Test acc 0.847756\n",
      "Epoch 17. Loss: 0.435903, Train acc 0.848875, Test acc 0.851562\n",
      "Epoch 18. Loss: 0.431521, Train acc 0.850461, Test acc 0.851763\n",
      "Epoch 19. Loss: 0.428323, Train acc 0.852163, Test acc 0.851663\n",
      "Epoch 20. Loss: 0.424029, Train acc 0.853065, Test acc 0.854868\n",
      "Epoch 21. Loss: 0.420908, Train acc 0.854517, Test acc 0.853766\n",
      "Epoch 22. Loss: 0.416837, Train acc 0.856303, Test acc 0.855970\n",
      "Epoch 23. Loss: 0.413880, Train acc 0.856520, Test acc 0.855869\n",
      "Epoch 24. Loss: 0.410836, Train acc 0.858540, Test acc 0.855268\n",
      "Epoch 25. Loss: 0.406982, Train acc 0.858507, Test acc 0.858774\n",
      "Epoch 26. Loss: 0.404709, Train acc 0.859208, Test acc 0.853165\n",
      "Epoch 27. Loss: 0.402087, Train acc 0.860594, Test acc 0.857873\n",
      "Epoch 28. Loss: 0.399623, Train acc 0.861862, Test acc 0.858073\n",
      "Epoch 29. Loss: 0.397118, Train acc 0.862480, Test acc 0.859776\n",
      "Epoch 30. Loss: 0.394114, Train acc 0.864183, Test acc 0.860076\n",
      "Epoch 31. Loss: 0.391718, Train acc 0.864416, Test acc 0.859776\n",
      "Epoch 32. Loss: 0.389629, Train acc 0.865735, Test acc 0.858674\n",
      "Epoch 33. Loss: 0.387226, Train acc 0.865869, Test acc 0.862079\n",
      "Epoch 34. Loss: 0.385263, Train acc 0.866703, Test acc 0.859776\n",
      "Epoch 35. Loss: 0.382494, Train acc 0.867788, Test acc 0.862680\n",
      "Epoch 36. Loss: 0.380805, Train acc 0.868406, Test acc 0.862981\n",
      "Epoch 37. Loss: 0.378393, Train acc 0.869124, Test acc 0.864884\n",
      "Epoch 38. Loss: 0.376012, Train acc 0.869775, Test acc 0.864283\n",
      "Epoch 39. Loss: 0.375001, Train acc 0.870576, Test acc 0.865084\n",
      "Epoch 40. Loss: 0.372573, Train acc 0.871244, Test acc 0.862881\n",
      "Epoch 41. Loss: 0.370464, Train acc 0.871761, Test acc 0.864784\n",
      "Epoch 42. Loss: 0.368581, Train acc 0.872746, Test acc 0.865885\n",
      "Epoch 43. Loss: 0.366875, Train acc 0.873231, Test acc 0.865385\n",
      "Epoch 44. Loss: 0.365394, Train acc 0.873781, Test acc 0.864884\n",
      "Epoch 45. Loss: 0.363620, Train acc 0.874065, Test acc 0.868490\n",
      "Epoch 46. Loss: 0.361784, Train acc 0.874783, Test acc 0.865785\n",
      "Epoch 47. Loss: 0.359980, Train acc 0.874800, Test acc 0.868289\n",
      "Epoch 48. Loss: 0.358734, Train acc 0.875835, Test acc 0.867388\n",
      "Epoch 49. Loss: 0.357055, Train acc 0.875885, Test acc 0.867588\n",
      "Epoch 50. Loss: 0.356274, Train acc 0.876319, Test acc 0.868890\n",
      "Epoch 51. Loss: 0.353514, Train acc 0.877437, Test acc 0.869091\n",
      "Epoch 52. Loss: 0.352678, Train acc 0.877304, Test acc 0.868089\n",
      "Epoch 53. Loss: 0.350825, Train acc 0.877971, Test acc 0.871795\n",
      "Epoch 54. Loss: 0.349189, Train acc 0.877888, Test acc 0.870393\n",
      "Epoch 55. Loss: 0.347828, Train acc 0.879657, Test acc 0.870393\n",
      "Epoch 56. Loss: 0.346255, Train acc 0.880909, Test acc 0.870693\n",
      "Epoch 57. Loss: 0.345566, Train acc 0.879941, Test acc 0.871394\n",
      "Epoch 58. Loss: 0.344407, Train acc 0.880375, Test acc 0.870693\n",
      "Epoch 59. Loss: 0.342147, Train acc 0.881227, Test acc 0.873898\n",
      "Epoch 60. Loss: 0.341136, Train acc 0.881043, Test acc 0.872997\n",
      "Epoch 61. Loss: 0.339309, Train acc 0.881844, Test acc 0.870192\n",
      "Epoch 62. Loss: 0.338395, Train acc 0.882078, Test acc 0.872696\n",
      "Epoch 63. Loss: 0.336947, Train acc 0.882212, Test acc 0.872796\n",
      "Epoch 64. Loss: 0.335672, Train acc 0.882696, Test acc 0.873998\n",
      "Epoch 65. Loss: 0.334758, Train acc 0.882979, Test acc 0.873297\n",
      "Epoch 66. Loss: 0.333355, Train acc 0.883931, Test acc 0.874800\n",
      "Epoch 67. Loss: 0.332194, Train acc 0.883747, Test acc 0.873097\n",
      "Epoch 68. Loss: 0.330996, Train acc 0.884582, Test acc 0.875401\n",
      "Epoch 69. Loss: 0.329362, Train acc 0.885600, Test acc 0.872596\n",
      "Epoch 70. Loss: 0.329124, Train acc 0.885033, Test acc 0.871294\n",
      "Epoch 71. Loss: 0.327312, Train acc 0.886769, Test acc 0.876502\n",
      "Epoch 72. Loss: 0.325894, Train acc 0.886769, Test acc 0.875801\n",
      "Epoch 73. Loss: 0.324939, Train acc 0.887220, Test acc 0.873397\n",
      "Epoch 74. Loss: 0.323783, Train acc 0.886952, Test acc 0.877504\n",
      "Epoch 75. Loss: 0.322366, Train acc 0.886986, Test acc 0.875601\n",
      "Epoch 76. Loss: 0.321287, Train acc 0.888088, Test acc 0.874900\n",
      "Epoch 77. Loss: 0.321068, Train acc 0.888338, Test acc 0.878906\n",
      "Epoch 78. Loss: 0.319214, Train acc 0.889056, Test acc 0.875300\n",
      "Epoch 79. Loss: 0.318307, Train acc 0.888672, Test acc 0.876302\n",
      "Epoch 80. Loss: 0.316757, Train acc 0.889456, Test acc 0.876002\n",
      "Epoch 81. Loss: 0.316415, Train acc 0.889456, Test acc 0.875601\n",
      "Epoch 82. Loss: 0.315372, Train acc 0.889941, Test acc 0.878105\n",
      "Epoch 83. Loss: 0.314383, Train acc 0.890375, Test acc 0.876903\n",
      "Epoch 84. Loss: 0.313383, Train acc 0.891309, Test acc 0.873397\n",
      "Epoch 85. Loss: 0.311700, Train acc 0.890875, Test acc 0.875801\n",
      "Epoch 86. Loss: 0.311091, Train acc 0.891777, Test acc 0.879006\n",
      "Epoch 87. Loss: 0.309722, Train acc 0.892111, Test acc 0.878305\n",
      "Epoch 88. Loss: 0.308333, Train acc 0.892261, Test acc 0.877804\n",
      "Epoch 89. Loss: 0.307187, Train acc 0.892845, Test acc 0.880208\n",
      "Epoch 90. Loss: 0.306803, Train acc 0.891810, Test acc 0.877905\n",
      "Epoch 91. Loss: 0.305163, Train acc 0.893596, Test acc 0.880008\n",
      "Epoch 92. Loss: 0.304654, Train acc 0.893446, Test acc 0.877905\n",
      "Epoch 93. Loss: 0.304229, Train acc 0.893530, Test acc 0.879207\n",
      "Epoch 94. Loss: 0.303440, Train acc 0.893263, Test acc 0.875701\n",
      "Epoch 95. Loss: 0.301813, Train acc 0.894131, Test acc 0.876302\n",
      "Epoch 96. Loss: 0.300900, Train acc 0.895149, Test acc 0.879307\n",
      "Epoch 97. Loss: 0.300107, Train acc 0.894965, Test acc 0.877604\n",
      "Epoch 98. Loss: 0.299378, Train acc 0.895049, Test acc 0.877304\n",
      "Epoch 99. Loss: 0.298389, Train acc 0.895516, Test acc 0.879006\n",
      "Epoch 100. Loss: 0.297358, Train acc 0.895783, Test acc 0.879307\n",
      "Epoch 101. Loss: 0.296151, Train acc 0.895800, Test acc 0.879107\n",
      "Epoch 102. Loss: 0.295748, Train acc 0.897019, Test acc 0.875701\n",
      "Epoch 103. Loss: 0.294760, Train acc 0.896985, Test acc 0.881210\n",
      "Epoch 104. Loss: 0.293761, Train acc 0.897720, Test acc 0.881110\n",
      "Epoch 105. Loss: 0.292237, Train acc 0.898154, Test acc 0.881410\n",
      "Epoch 106. Loss: 0.291895, Train acc 0.897469, Test acc 0.876302\n",
      "Epoch 107. Loss: 0.291057, Train acc 0.897720, Test acc 0.881210\n",
      "Epoch 108. Loss: 0.289667, Train acc 0.898888, Test acc 0.881510\n",
      "Epoch 109. Loss: 0.289382, Train acc 0.899055, Test acc 0.880709\n",
      "Epoch 110. Loss: 0.287816, Train acc 0.899539, Test acc 0.879908\n",
      "Epoch 111. Loss: 0.287605, Train acc 0.899272, Test acc 0.880809\n",
      "Epoch 112. Loss: 0.286948, Train acc 0.899556, Test acc 0.882212\n",
      "Epoch 113. Loss: 0.285819, Train acc 0.899422, Test acc 0.880308\n",
      "Epoch 114. Loss: 0.284450, Train acc 0.900324, Test acc 0.879607\n",
      "Epoch 115. Loss: 0.284773, Train acc 0.899923, Test acc 0.882312\n",
      "Epoch 116. Loss: 0.284022, Train acc 0.900107, Test acc 0.876002\n",
      "Epoch 117. Loss: 0.282405, Train acc 0.901392, Test acc 0.883914\n",
      "Epoch 118. Loss: 0.281698, Train acc 0.901776, Test acc 0.882612\n",
      "Epoch 119. Loss: 0.280888, Train acc 0.902127, Test acc 0.884615\n",
      "Epoch 120. Loss: 0.279917, Train acc 0.902828, Test acc 0.881310\n",
      "Epoch 121. Loss: 0.279851, Train acc 0.902093, Test acc 0.882212\n",
      "Epoch 122. Loss: 0.279149, Train acc 0.902961, Test acc 0.883614\n",
      "Epoch 123. Loss: 0.277393, Train acc 0.903512, Test acc 0.882011\n",
      "Epoch 124. Loss: 0.277174, Train acc 0.904030, Test acc 0.878506\n",
      "Epoch 125. Loss: 0.276827, Train acc 0.902978, Test acc 0.885016\n",
      "Epoch 126. Loss: 0.275557, Train acc 0.904230, Test acc 0.884014\n",
      "Epoch 127. Loss: 0.274222, Train acc 0.903863, Test acc 0.880909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128. Loss: 0.273452, Train acc 0.903930, Test acc 0.883213\n",
      "Epoch 129. Loss: 0.273573, Train acc 0.903963, Test acc 0.881210\n",
      "Epoch 130. Loss: 0.272899, Train acc 0.904514, Test acc 0.880909\n",
      "Epoch 131. Loss: 0.271742, Train acc 0.905248, Test acc 0.883113\n",
      "Epoch 132. Loss: 0.271635, Train acc 0.905766, Test acc 0.884415\n",
      "Epoch 133. Loss: 0.270552, Train acc 0.905365, Test acc 0.885617\n",
      "Epoch 134. Loss: 0.269568, Train acc 0.906684, Test acc 0.885417\n",
      "Epoch 135. Loss: 0.270344, Train acc 0.905382, Test acc 0.885216\n",
      "Epoch 136. Loss: 0.267944, Train acc 0.906250, Test acc 0.884515\n",
      "Epoch 137. Loss: 0.267255, Train acc 0.907018, Test acc 0.884315\n",
      "Epoch 138. Loss: 0.267002, Train acc 0.906367, Test acc 0.886118\n",
      "Epoch 139. Loss: 0.265952, Train acc 0.906717, Test acc 0.885116\n",
      "Epoch 140. Loss: 0.265413, Train acc 0.906667, Test acc 0.885617\n",
      "Epoch 141. Loss: 0.265008, Train acc 0.907502, Test acc 0.884515\n",
      "Epoch 142. Loss: 0.264016, Train acc 0.908019, Test acc 0.883614\n",
      "Epoch 143. Loss: 0.264104, Train acc 0.907435, Test acc 0.883914\n",
      "Epoch 144. Loss: 0.262620, Train acc 0.908921, Test acc 0.887921\n",
      "Epoch 145. Loss: 0.261595, Train acc 0.908337, Test acc 0.881711\n",
      "Epoch 146. Loss: 0.260797, Train acc 0.909706, Test acc 0.887420\n",
      "Epoch 147. Loss: 0.261308, Train acc 0.908470, Test acc 0.887620\n",
      "Epoch 148. Loss: 0.259856, Train acc 0.909205, Test acc 0.888421\n",
      "Epoch 149. Loss: 0.259280, Train acc 0.908971, Test acc 0.887119\n",
      "Epoch 150. Loss: 0.258517, Train acc 0.909438, Test acc 0.887019\n",
      "Epoch 151. Loss: 0.258239, Train acc 0.909856, Test acc 0.887420\n",
      "Epoch 152. Loss: 0.257210, Train acc 0.910240, Test acc 0.888121\n",
      "Epoch 153. Loss: 0.256368, Train acc 0.910690, Test acc 0.886819\n",
      "Epoch 154. Loss: 0.255912, Train acc 0.910473, Test acc 0.888221\n",
      "Epoch 155. Loss: 0.255401, Train acc 0.910423, Test acc 0.889123\n",
      "Epoch 156. Loss: 0.253872, Train acc 0.911993, Test acc 0.887720\n",
      "Epoch 157. Loss: 0.253238, Train acc 0.911792, Test acc 0.890625\n",
      "Epoch 158. Loss: 0.252851, Train acc 0.911425, Test acc 0.885517\n",
      "Epoch 159. Loss: 0.252967, Train acc 0.911258, Test acc 0.888221\n",
      "Epoch 160. Loss: 0.252027, Train acc 0.912844, Test acc 0.889724\n",
      "Epoch 161. Loss: 0.251286, Train acc 0.912109, Test acc 0.888321\n",
      "Epoch 162. Loss: 0.250196, Train acc 0.912627, Test acc 0.886418\n",
      "Epoch 163. Loss: 0.250193, Train acc 0.912276, Test acc 0.889022\n",
      "Epoch 164. Loss: 0.249055, Train acc 0.913111, Test acc 0.890124\n",
      "Epoch 165. Loss: 0.248724, Train acc 0.913845, Test acc 0.885917\n",
      "Epoch 166. Loss: 0.248531, Train acc 0.913361, Test acc 0.887821\n",
      "Epoch 167. Loss: 0.247906, Train acc 0.913128, Test acc 0.887720\n",
      "Epoch 168. Loss: 0.247310, Train acc 0.913762, Test acc 0.888522\n",
      "Epoch 169. Loss: 0.246028, Train acc 0.913979, Test acc 0.888822\n",
      "Epoch 170. Loss: 0.246650, Train acc 0.913261, Test acc 0.890124\n",
      "Epoch 171. Loss: 0.245313, Train acc 0.914196, Test acc 0.890825\n",
      "Epoch 172. Loss: 0.245528, Train acc 0.914113, Test acc 0.889423\n",
      "Epoch 173. Loss: 0.244051, Train acc 0.914764, Test acc 0.884515\n",
      "Epoch 174. Loss: 0.243724, Train acc 0.914814, Test acc 0.888421\n",
      "Epoch 175. Loss: 0.243324, Train acc 0.914864, Test acc 0.890425\n",
      "Epoch 176. Loss: 0.241898, Train acc 0.915281, Test acc 0.885016\n",
      "Epoch 177. Loss: 0.241559, Train acc 0.915682, Test acc 0.888522\n",
      "Epoch 178. Loss: 0.241036, Train acc 0.915865, Test acc 0.890425\n",
      "Epoch 179. Loss: 0.240370, Train acc 0.916950, Test acc 0.890124\n",
      "Epoch 180. Loss: 0.239692, Train acc 0.917134, Test acc 0.885016\n",
      "Epoch 181. Loss: 0.239677, Train acc 0.916917, Test acc 0.892127\n",
      "Epoch 182. Loss: 0.238481, Train acc 0.917668, Test acc 0.886819\n",
      "Epoch 183. Loss: 0.238154, Train acc 0.917368, Test acc 0.890825\n",
      "Epoch 184. Loss: 0.237714, Train acc 0.917334, Test acc 0.887720\n",
      "Epoch 185. Loss: 0.236660, Train acc 0.917334, Test acc 0.889423\n",
      "Epoch 186. Loss: 0.235797, Train acc 0.918202, Test acc 0.890325\n",
      "Epoch 187. Loss: 0.235868, Train acc 0.918753, Test acc 0.891126\n",
      "Epoch 188. Loss: 0.234481, Train acc 0.918803, Test acc 0.888822\n",
      "Epoch 189. Loss: 0.235027, Train acc 0.917702, Test acc 0.890625\n",
      "Epoch 190. Loss: 0.235177, Train acc 0.917802, Test acc 0.889623\n",
      "Epoch 191. Loss: 0.234229, Train acc 0.918770, Test acc 0.890725\n",
      "Epoch 192. Loss: 0.232397, Train acc 0.919772, Test acc 0.889523\n",
      "Epoch 193. Loss: 0.232278, Train acc 0.919154, Test acc 0.890525\n",
      "Epoch 194. Loss: 0.231305, Train acc 0.919722, Test acc 0.891126\n",
      "Epoch 195. Loss: 0.231878, Train acc 0.919621, Test acc 0.891226\n",
      "Epoch 196. Loss: 0.230752, Train acc 0.920356, Test acc 0.889924\n",
      "Epoch 197. Loss: 0.230436, Train acc 0.920122, Test acc 0.892328\n",
      "Epoch 198. Loss: 0.229585, Train acc 0.920623, Test acc 0.889724\n",
      "Epoch 199. Loss: 0.229298, Train acc 0.920122, Test acc 0.889123\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd\n",
    "import utils\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "train_data, test_data = utils.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.02})\n",
    "\n",
    "for epoch in range(200):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    for data, label in train_data:\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "\n",
    "        train_loss += nd.mean(loss).asscalar()\n",
    "        train_acc += utils.accuracy(output, label)\n",
    "\n",
    "    test_acc = utils.evaluate_accuracy(test_data, net)\n",
    "    print(\"Epoch %d. Loss: %f, Train acc %f, Test acc %f\" % (\n",
    "        epoch, train_loss/len(train_data), train_acc/len(train_data), test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
